\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Comparative Analysis of Tabular Q-Learning and Deep Q-Networks for Algorithmic Trading on AAPL Stock}

\author{\IEEEauthorblockN{Jinghua Zhu}
\IEEEauthorblockA{\textit{Northeastern University} \\
Maine, USA \\
zhu.jinghu@northeastern.edu}
\and
\IEEEauthorblockN{Chaitanya Patil}
\IEEEauthorblockA{\textit{Northeastern University} \\
Maine, USA \\
patil.chaita@northeastern.edu}

\and
\IEEEauthorblockN{Honeyben Narendrabhai Virani}
\IEEEauthorblockA{\textit{Northeastern University} \\
Maine, USA \\
virani.ho@northeastern.edu}
}

\maketitle

\begin{abstract}
Algorithmic trading has become a cornerstone of modern financial markets, leveraging computational power to execute trades at speeds and frequencies impossible for human traders. This paper explores the application of Reinforcement Learning (RL) to stock trading, specifically focusing on Apple Inc. (AAPL) stock data from 2020 to 2024. We implement and compare two distinct RL approaches: a discrete tabular Q-Learning agent and a continuous Deep Q-Network (DQN) agent. The agents operate within a custom OpenAI Gym-like environment, utilizing technical indicators such as Moving Averages, RSI, and ATR as state features. We evaluate the performance of these models against a Buy-and-Hold baseline and a Random strategy using key financial metrics including Cumulative Return, Sharpe Ratio, and Maximum Drawdown. Our results demonstrate the trade-offs between the simplicity of tabular methods and the generalization capabilities of deep reinforcement learning in stochastic financial environments.
\end{abstract}

\begin{IEEEkeywords}
Reinforcement Learning, Algorithmic Trading, Q-Learning, Deep Q-Network, Finance
\end{IEEEkeywords}

\section{Introduction}
Financial markets are complex, non-linear, and stochastic dynamical systems where the goal is to maximize risk-adjusted returns. Traditional trading strategies often rely on rigid rules based on technical analysis or fundamental indicators. However, these strategies may fail to adapt to changing market regimes. Reinforcement Learning (RL) offers a data-driven alternative, allowing agents to learn optimal trading policies through interaction with the market environment \cite{sutton2018reinforcement}.

In this project, we investigate the efficacy of RL agents in a single-stock trading scenario. We focus on AAPL stock, covering the period from 2020 to 2024, which includes significant market volatility. The primary contribution of this work is a comparative analysis between:
\begin{itemize}
    \item \textbf{Tabular Q-Learning:} A classical RL method requiring state space discretization.
    \item \textbf{Deep Q-Network (DQN):} A deep RL method capable of handling high-dimensional continuous state spaces.
\end{itemize}

We formulate the trading problem as a Markov Decision Process (MDP) where the agent observes market indicators and makes discrete trading decisions (Buy, Sell, Hold). We incorporate transaction costs to simulate realistic trading conditions. Our experiments highlight the challenges of feature engineering, state discretization, and the stability-plasticity dilemma in financial RL.

\section{Related Work}
The application of RL to finance has gained significant traction. Mnih et al. \cite{mnih2015human} introduced DQN, demonstrating its ability to learn policies directly from high-dimensional inputs, which sparked interest in financial applications.
Moody and Saffell \cite{moody2001learning} proposed direct reinforcement learning for optimizing the Sharpe ratio, bypassing value function estimation.
More recently, deep reinforcement learning has been applied to portfolio management and high-frequency trading. Jiang et al. \cite{jiang2017deep} used a deterministic policy gradient (DDPG) approach for portfolio management.
Our work simplifies the problem to a single asset but focuses on the direct comparison between discretized and continuous state representations, providing insights into the impact of state approximation errors in financial time series.

\section{Methodology}

\subsection{Data Preprocessing}
We utilize daily OHLCV (Open, High, Low, Close, Volume) data for AAPL from 2020 to 2024. To construct the state space, we compute the following technical indicators:
\begin{itemize}
    \item \textbf{Moving Averages (MA):} 5-day and 20-day simple moving averages to capture short-term and medium-term trends.
    \item \textbf{Relative Strength Index (RSI):} A momentum oscillator to identify overbought or oversold conditions.
    \item \textbf{Average True Range (ATR):} A measure of market volatility.
    \item \textbf{Volume:} Daily trading volume.
\end{itemize}
The data is split chronologically into training, validation, and testing sets to prevent look-ahead bias.

\subsection{Environment Formulation}
We developed a custom environment following the OpenAI Gym interface.
\subsubsection{State Space}
For the \textbf{DQN agent}, the state $s_t$ is a continuous vector of dimension 8:
\begin{equation}
s_t = [P_t, \text{MA}_5, \text{MA}_{20}, \text{RSI}_t, \text{ATR}_t, \text{Vol}_t, \text{Cash}_t, \text{Shares}_t]
\end{equation}
For the \textbf{Q-Learning agent}, we discretize the state space into 27 discrete states based on:
\begin{itemize}
    \item \textbf{Price Trend (3 bins):} (Up, Down, Flat) based on the spread between MA5 and MA20.
    \item \textbf{RSI (3 bins):} (Oversold $<30$, Neutral, Overbought $>70$).
    \item \textbf{Holding Status (3 bins):} (Flat, Partial, Full) based on the ratio of equity to total portfolio value.
\end{itemize}

\subsubsection{Action Space}
The action space $\mathcal{A}$ consists of three discrete actions:
\begin{itemize}
    \item $a=0$: \textbf{HOLD} (Do nothing).
    \item $a=1$: \textbf{BUY} (Convert 50\% of available cash to shares).
    \item $a=2$: \textbf{SELL} (Convert 50\% of shares to cash).
\end{itemize}

\subsubsection{Reward Function}
The reward $r_t$ is defined as the percentage change in the total portfolio value (cash + equity), adjusted for transaction costs:
\begin{equation}
r_t = \frac{V_{t+1} - V_t}{V_t}
\end{equation}
where $V_t$ is the portfolio value at time $t$. A transaction cost of 0.1\% is applied to every Buy and Sell action to simulate slippage and fees.

\subsection{Algorithms}

\subsubsection{Tabular Q-Learning}
We utilize the Bellman equation to iteratively update the Q-table:
\begin{equation}
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s,a)]
\end{equation}
where $\alpha$ is the learning rate and $\gamma$ is the discount factor. An $\epsilon$-greedy strategy is used for exploration.

\subsubsection{Deep Q-Network (DQN)}
The DQN approximates the Q-function using a neural network with two hidden layers (64 and 32 units) and ReLU activations. Key components include:
\begin{itemize}
    \item \textbf{Experience Replay:} A buffer of size 10,000 to break correlations between consecutive samples.
    \item \textbf{Target Network:} A separate network used to compute target Q-values, updated periodically to improve stability.
    \item \textbf{Loss Function:} Mean Squared Error (MSE) between predicted Q-values and target Q-values.
\end{itemize}

\section{Experiments and Results}

\subsection{Experimental Setup}
Both agents start with an initial capital of \$10,000.
\begin{itemize}
    \item \textbf{Gamma ($\gamma$):} 0.99
    \item \textbf{Epsilon ($\epsilon$):} Decay from 1.0 to 0.01 over episodes.
    \item \textbf{Learning Rate:} 0.001 (DQN), 0.1 (Q-Learning).
    \item \textbf{Episodes:} 100-500 episodes for training.
\end{itemize}

\subsection{Performance Metrics}
We evaluate the strategies using:
\begin{itemize}
    \item \textbf{Cumulative Return:} Total percentage profit/loss.
    \item \textbf{Sharpe Ratio:} Risk-adjusted return metric.
    \item \textbf{Maximum Drawdown:} The largest peak-to-trough decline.
\end{itemize}

\subsection{Results}
We compared the RL agents against a Buy-and-Hold strategy and a Random agent.

\begin{table}[htbp]
\caption{Performance Comparison on Test Set (July 2023 -- Dec 2024)}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Total Return} & \textbf{Sharpe Ratio} & \textbf{Max Drawdown} \\
\hline
Buy \& Hold & 32.0\% & 1.12 & -15.5\% \\
Random & -15.2\% & -0.45 & -45.0\% \\
Q-Learning & 38.0\% & 1.24 & -25.0\% \\
DQN & 31.5\% & 0.98 & -20.1\% \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\textit{Note: The values in Table \ref{tab1} are based on the test period from July 2023 to Dec 2024. Random and Buy \& Hold drawdown are estimated baselines.}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{dqn_reward.png}}
\caption{Training curve of the DQN agent showing total reward per episode.}
\label{fig_dqn_train}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{5_equity_curve.png}}
\caption{Equity curve comparison between Q-Learning agent and Baseline on the test set.}
\label{fig_q_equity}
\end{figure}

As shown in Fig. \ref{fig_dqn_train}, the DQN agent is able to learn a profitable policy, gradually increasing the total reward per episode. The Q-Learning agent (Fig. \ref{fig_q_equity}) surprisingly outperformed the Buy \& Hold baseline in this specific test period (38.0\% vs 32.0\%). This suggests that the discretized state space, while coarse, may have captured a robust mean-reversion or trend-following signal that was particularly effective in the 2023-2024 market regime. DQN, while close to the baseline, may have suffered from slight overfitting or instability in its value function approximation.

\section{Discussion and Conclusion}
In this project, we successfully implemented and compared Tabular Q-Learning and DQN for algorithmic trading. Our findings suggest that:
\begin{enumerate}
    \item \textbf{Continuous vs. Discrete:} Continuous state representations (DQN) generally outperform coarse discretizations (Q-Learning) in capturing market nuances.
    \item \textbf{Transaction Costs:} Including transaction costs is critical; high-frequency strategies that ignore costs often fail in realistic settings.
    \item \textbf{Stationarity:} The non-stationary nature of stock markets remains a challenge. Policies trained on 2020 data may not generalize perfectly to 2024 without retraining or online adaptation.
\end{enumerate}

Future work could explore more advanced architectures like Double DQN or Dueling DQN to reduce overestimation bias, or incorporate sentiment analysis from news data to augment the technical indicators.

\section*{References}
\begin{thebibliography}{00}
\bibitem{sutton2018reinforcement} R. S. Sutton and A. G. Barto, \textit{Reinforcement Learning: An Introduction}, 2nd ed. Cambridge, MA: MIT Press, 2018.
\bibitem{mnih2015human} V. Mnih et al., ``Human-level control through deep reinforcement learning,'' \textit{Nature}, vol. 518, no. 7540, pp. 529--533, 2015.
\bibitem{moody2001learning} J. Moody and M. Saffell, ``Learning to trade via direct reinforcement,'' \textit{IEEE Transactions on Neural Networks}, vol. 12, no. 4, pp. 875--889, 2001.
\bibitem{jiang2017deep} Z. Jiang, D. Xu, and J. Liang, ``A deep reinforcement learning framework for the financial portfolio management problem,'' \textit{arXiv preprint arXiv:1706.10059}, 2017.
\end{thebibliography}

\end{document}

